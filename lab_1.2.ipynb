{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Lab 1.2\n",
        "author: Landon Carpenter\n",
        "code-fold: true\n",
        "self-contained: true\n",
        "output: lab1.2.html\n",
        "---"
      ],
      "id": "33db9bff"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backpropagation\n",
        "\n",
        "**Problem-1:**. In this problem, you will write code to perform backpropagation from scratch. Consider a network with 2 inputs, 1 hidden layer containing 3 units, and a single categorical output.\n",
        "\n",
        "Imagine the two input nodes are labeled $x_1$ and $x_2$, the three hidden nodes are $a_1$, $a_2$, and $a_3$, and the output node is $\\hat{y}$. The edges from the input layer to the hidden layer are labeled $w_{ij}$ for going from input $x_i$ to hidden node $a_j$, and the edges from the hidden layer to the output layer are $v_j$, from hidden unit $a_j$ to $\\hat{y}$. There is a bias vector for the hidden layer $\\mathbf{b}$ and a bias constant $c$ for the output layer.\n",
        "\n",
        "Let $a_j = \\max(0, z_j)$, where $z_j$ is a weighted sum from the previous layer plus a bias unit. That is,\n",
        "$$\n",
        "z_1 = w_{11}x_1 + w_{21}x_2 + b_1 \\\\\n",
        "z_2 = w_{12}x_1 + w_{22}x_2 + b_2 \\\\\n",
        "z_3 = w_{13}x_1 + w_{23}x_2 + b_3 \\\\\n",
        "\\Rightarrow z_j = w_{1j}x_1 + w_{2j}x_2 + b_j\n",
        "$$\n",
        "\n",
        "For the output, we write\n",
        "\n",
        "$$ \\hat{y} = g(v_1a_1 + v_2a_2 + v_3a_3 + c), $$\n",
        "where $g$ is the output function (in this case, for binary classification, $g$ is the sigmoid function). Expanding the above expression to show $\\hat{y}$ as a function of all of the variables of the graph, we obtain\n",
        "$$ \\hat{y} = g\\big[v_1\\max(0, w_{11}x_1 + w_{21}x_2 + b_1) \\\\ + v_2\\max(0, w_{12}x_1 + w_{22}x_2 + b_2) \\\\ + v_3\\max(0, w_{13}x_1 + w_{23}x_2 + b_3) + c\\large].$$\n",
        "\n",
        "We can express this succinctly using matrix notation. If\n",
        "\n",
        "$$ W = \\begin{bmatrix}\n",
        "w_{11} &w_{12} &w_{13}\\\\\n",
        "w_{21} &w_{22} &w_{23}\\\\\n",
        "\\end{bmatrix}, \\hspace{.5cm} \\mathbf{x} = \\begin{bmatrix} x_1 \\\\x_2 \\end{bmatrix}, \\hspace{.5cm} \\mathbf{b} = \\begin{bmatrix} b_1 \\\\b_2 \\\\b_3\\end{bmatrix}, \\hspace{.5cm} \\mathbf{a} = \\begin{bmatrix} a_1 \\\\a_2 \\\\a_3\\end{bmatrix}, \\hspace{.5cm} \\text{and} \\hspace{.5cm} \\mathbf{v} = \\begin{bmatrix} v_1 \\\\v_2 \\\\v_3\\end{bmatrix},$$\n",
        "\n",
        "then\n",
        "$$\n",
        "z_j = W^{\\text{T}}\\mathbf{x} + \\mathbf{b}, \\hspace{.5cm} a_j = \\max(0, z_j), \\hspace{.5cm} \\text{and} \\hspace{.5cm} \\hat{y} = g(\\mathbf{v}^{\\text{T}}\\cdot\\mathbf{a} + c).\n",
        "$$\n",
        "\n",
        "(A) Derive expressions of the gradient of the loss function with respect to each of the model parameters.\n",
        "\n",
        "(B) Write a function `grad_f(...)` that takes in a weights vector and returns the gradient of the loss at that location. You will also need to write a number of helper functions.\n",
        "\n",
        "(C) Generate a synthetic dataset resembling an XOR pattern. This has been done for you.\n",
        "\n",
        "(D) Fit the network with gradient descent. Keep track of the total loss at each iteration. Create a plot of the loss over training iterations.\n",
        "\n",
        "(E) Repeat (D) but using momentum. Compare and contrast the results.\n",
        "\n",
        "(F) Plot a visualization of the final decision boundary, and overlay the decision boundary over the XOR dataset you created in (C).\n",
        "\n",
        "`Below we provide some starter code to get you started`\n"
      ],
      "id": "b01a3e21"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np"
      ],
      "id": "dd89197f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**YOUR ANSWER FOR (A) HERE**:\n"
      ],
      "id": "0428d06a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#define sigmoid activation \n",
        "def sig(x):\n",
        "    return 1.0 / (1.0+ np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "#define relu activation\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "#determine the derivative of relu function, either 1 or 0\n",
        "def relu_derivative(z):\n",
        "    return (z > 0).astype(int)\n",
        "\n",
        "#now we can create a function to compute the gradiant of L with respect to W and b using model parameters defined in problem 1\n",
        "def get_grad(x, y, W, b, v, c):\n",
        "    #forward pass\n",
        "    z = np.dot(W.T, x) + b #get weighted sum in hidden layer\n",
        "    a = relu(z) #use relu function created earlier\n",
        "    y_hat = sig(np.dot(v.T, a) + c) #get the output of the model\n",
        "\n",
        "    #backward pass\n",
        "    dL_dyhat = -y / y_hat + (1 - y) / (1 - y_hat) #find the derivative of L with respect to y_hat\n",
        "\n",
        "    dyhat_dv = a #y_hat is linear combination of a's, so derivative is just a\n",
        "    dyhat_dc = 1 #y_hat is linear combination of 1's, so derivative is just 1\n",
        "\n",
        "    #get gradients of L for v and c\n",
        "    dL_dv = dL_dyhat * dyhat_dv #chain rule to find derivative of L with respect to v\n",
        "    dL_dc = dL_dyhat * dyhat_dc #chain rule to find derivative of L with respect to c\n",
        "\n",
        "    #compute gradient of L with respect to a (activations of hidden layer)\n",
        "    dL_da = np.dot(v, dL_dyhat) #use chain rule to find derivative of L with respect to a\n",
        "    dL_dz = dL_da *relu_derivative(z)  #use the derivative of relu function created earlier\n",
        "\n",
        "    #compute gradients of L with respect to W and b using chain rule\n",
        "    dL_dW = np.dot(x, dL_dz.T) #make sure dimensions match\n",
        "    dL_db = dL_dz\n",
        "\n",
        "    return dL_dW, dL_db, dL_dv, dL_dc"
      ],
      "id": "e398f996",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Code for (B) here\n"
      ],
      "id": "d077fd6d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#define sigmoid, relu and their derivatives\n",
        "sigmoid = lambda x: 1.0 / (1.0 + np.exp(-np.clip(x, -500, 500)))\n",
        "relu = lambda z: np.maximum(0, z)\n",
        "sigmoid_prime = lambda x: sigmoid(x) * (1 - sigmoid(x))\n",
        "relu_prime = lambda z: (z > 0).astype(int)\n",
        "\n",
        "#define loss function and its derivative\n",
        "def loss(y, y_hat):\n",
        "    epsilon = 1e-10  #add small value to prevent log(0)\n",
        "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)  #clip y_hat to prevent log(0)\n",
        "    #compute binary cross-entropy loss\n",
        "    return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
        "\n",
        "def loss_derivative(y, y_hat):\n",
        "    epsilon = 1e-10  #add small value to prevent division by 0\n",
        "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)  #clip y_hat to prevent division by 0\n",
        "    #compute derivative of binary cross-entropy loss\n",
        "    return -y / y_hat + (1 - y) / (1 - y_hat)\n",
        "\n",
        "#define function to compute y_hat\n",
        "def yhat(X, params):\n",
        "    W, b, v, c = params\n",
        "    #forward pass to compute y_hat\n",
        "    z = np.dot(W.T, X.T) + b  #weighted sum in hidden layer\n",
        "    a = relu(z)  #apply relu activation\n",
        "    y_hat = sigmoid(np.dot(v.T, a) + c)  #compute output of the model\n",
        "    return y_hat\n",
        "\n",
        "#define function to compute gradients\n",
        "def grad_f(params, x, y):\n",
        "    W, b, v, c = params\n",
        "    #forward pass\n",
        "    z = np.dot(W.T, x.T) + b.reshape(-1, 1)  #calculate weighted sum in hidden layer\n",
        "    a = relu(z)  #apply relu activation\n",
        "    y_hat = sigmoid(np.dot(v.T, a) + c)  #calculate output of the model\n",
        "\n",
        "    # #print shapes of all variables\n",
        "    # print(f\"z shape: {z.shape}\\na shape: {a.shape}\\ny_hat shape: {y_hat.shape}\")\n",
        "\n",
        "    #backward pass\n",
        "    dL_dyhat = loss_derivative(y.T, y_hat)\n",
        "\n",
        "    dyhat_dv = a  #derivative with respect to v is a\n",
        "    dyhat_dc = 1  #derivative with respect to c is 1\n",
        "\n",
        "    # print(f\"dL_dyhat shape: {dL_dyhat.shape}\\n\")\n",
        "\n",
        "    #compute gradients of L with respect to v and c\n",
        "    dL_dv = np.dot(dyhat_dv, dL_dyhat.T) #apply chain rule for v\n",
        "    dL_dc = np.sum(dL_dyhat)  #apply chain rule for c\n",
        "\n",
        "    # #print shapes of all variables\n",
        "    # print(f\"dL_dv shape: {dL_dv.shape}\\ndL_dc shape: {dL_dc.shape}\")\n",
        "\n",
        "    # #print shapes of v and dL_dyhat\n",
        "    # print(f\"v shape: {v.shape}\\ndL_dyhat shape: {dL_dyhat.shape}\")\n",
        "\n",
        "    #compute gradient of L with respect to a using chain rule\n",
        "    dL_da = np.dot(v, dL_dyhat)  #apply chain rule for a\n",
        "\n",
        "    # #print shapes of dL_da and z\n",
        "    # print(f\"dL_da shape: {dL_da.shape}\\nz shape: {z.shape}\")\n",
        "\n",
        "    #compute gradient of L with respect to z (activation of hidden layer)\n",
        "    dL_dz = np.multiply(dL_da, relu_prime(z))  #apply chain rule for z\n",
        "\n",
        "    # #print shapes of dL_dz and x\n",
        "    # print(f\"dL_dz shape: {dL_dz.shape}\\nx shape: {x.shape}\")\n",
        "\n",
        "    #compute gradients of L with respect to W and b using chain rule\n",
        "    dL_dW = np.dot(x.T, dL_dz.T)  #ensure dimensionality matches\n",
        "    dL_db = np.sum(dL_dz, axis=1)  #sum over axis if batched\n",
        "\n",
        "    # #print shapes of dL_dW and dL_db\n",
        "    # print(f\"dL_dW shape: {dL_dW.shape}\\ndL_db shape: {dL_db.shape}\")\n",
        "\n",
        "    return [dL_dW, dL_db, dL_dv, dL_dc]"
      ],
      "id": "e6af4432",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from matplotlib import pyplot as plt"
      ],
      "id": "60f1d3bc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Code for (C) here\n",
        "\n",
        "np.random.seed(12345)\n",
        "\n",
        "Xs = np.random.uniform(low=-2.0, high=2.0, size=1000).reshape((1000, 1))\n",
        "Ys = np.random.uniform(low=-2.0, high=2.0, size=1000).reshape((1000, 1))\n",
        "Zs = np.logical_xor(Xs<0, Ys>0).reshape((1000, 1))\n",
        "\n",
        "plt.scatter(Xs, Ys, c=Zs)\n",
        "plt.show()"
      ],
      "id": "4f209d81",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def gradient_descent(x, y, starting_params, iterations, lr):\n",
        "    params = starting_params\n",
        "    W, b, v, c = params\n",
        "    losses = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "        grad = grad_f(params, x, y)  #compute gradients\n",
        "        dL_dW, dL_db, dL_dv, dL_dc = grad\n",
        "\n",
        "        #print the shape of parameters\n",
        "        # print(f\"W shape: {W.shape}\")\n",
        "        b = b.reshape(-1, 1)\n",
        "        # print(f\"b shape: {b.shape}\")\n",
        "        # print(f\"v shape: {v.shape}\")\n",
        "\n",
        "        # print(f\"dL_dW shape: {dL_dW.shape}\")\n",
        "        # print(f\"dL_db shape: {dL_db.shape}\")\n",
        "        # print(f\"dL_dv shape: {dL_dv.shape}\")\n",
        "\n",
        "        #update each parameter\n",
        "        \n",
        "        W -= lr * dL_dW\n",
        "        b -= lr * dL_db\n",
        "        v -= lr * dL_dv\n",
        "        c -= lr * dL_dc\n",
        "        \n",
        "\n",
        "        params = [W, b, v, c]\n",
        "\n",
        "        y_hat = yhat(x, params)  #compute y_hat\n",
        "        current_loss = np.mean(loss(y, y_hat))  #compute loss\n",
        "        losses.append(current_loss)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}, Loss: {current_loss}\")\n",
        "\n",
        "    return params, losses"
      ],
      "id": "1fb06653",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Code for (B) here\n",
        "\n",
        "np.random.seed(123456)\n",
        "\n",
        "W_init = np.random.rand(2,3)-0.01\n",
        "b_init = np.random.rand(3)-0.01\n",
        "v_init = np.random.rand(3,1)-0.01\n",
        "c_init = np.random.rand()-0.01\n",
        "\n",
        "params_init = [W_init, b_init, v_init, c_init]\n",
        "\n",
        "print(f\"W is {W_init}\\nb is {b_init}\\nv is {v_init}\\nc is {c_init}\")\n",
        "\n",
        "xs = np.random.uniform(low=-2, high=2, size=1000).reshape((500,2))\n",
        "ys = np.zeros(500)\n",
        "ys[np.logical_and(xs[:,0]>0, xs[:,1]>0)]=1\n",
        "ys[np.logical_and(xs[:,0]<0, xs[:,1]<0)]=1\n",
        "\n",
        "xs = np.asmatrix(xs)\n",
        "ys = np.asmatrix(ys).reshape((500,1))\n",
        "\n",
        "trajectories_standard, losses_standard = gradient_descent(xs, ys, params_init,\n",
        "                                        iterations=3000, lr=1e-6)\n",
        "plt.plot(losses_standard)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Gradient Descent\")\n",
        "plt.show()"
      ],
      "id": "e36f656e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I've been noticing that after a while the loss goes up depending on the learning rate. Everything is tuned right now to work, but early stopping would be a good idea. I think its out of the scope of this lab though.\n"
      ],
      "id": "2f18d6c0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your code for (E) here\n",
        "\n",
        "def gradient_descent_momentum(x, y, starting_params, iterations, lr, alpha):\n",
        "    params = starting_params\n",
        "    W, b, v, c = params\n",
        "\n",
        "    #initialize velocity terms for momentum\n",
        "    vW = np.zeros_like(W)\n",
        "    vb = np.zeros_like(b)\n",
        "    vb = vb.reshape(-1, 1)\n",
        "    vv = np.zeros_like(v)\n",
        "    vc = 0\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "        grad = grad_f(params, x, y)  #compute gradients\n",
        "        dL_dW, dL_db, dL_dv, dL_dc = grad\n",
        "        \n",
        "        #apply momentum\n",
        "        vW = alpha * vW + lr * dL_dW\n",
        "        vb = alpha * vb + lr * dL_db\n",
        "        vv = alpha * vv + lr * dL_dv\n",
        "        vc = alpha * vc + lr * dL_dc\n",
        "\n",
        "        #update parameters with momentum term\n",
        "        W -= vW\n",
        "        b = b.reshape(-1, 1)\n",
        "        b -= vb\n",
        "        v -= vv\n",
        "        c -= vc\n",
        "\n",
        "        params = [W, b, v, c]\n",
        "\n",
        "        y_hat = yhat(x, params)  #compute y_hat\n",
        "        current_loss = np.mean(loss(y, y_hat))  #compute loss\n",
        "        losses.append(current_loss)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}, Loss: {current_loss}\")\n",
        "\n",
        "    return params, losses"
      ],
      "id": "20b54208",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "W_init = np.random.rand(2,3)-0.01\n",
        "b_init = np.random.rand(3)-0.01\n",
        "v_init = np.random.rand(3,1)-0.01\n",
        "c_init = np.random.rand()-0.01\n",
        "\n",
        "params_init = [W_init, b_init, v_init, c_init]\n",
        "\n",
        "xs = np.random.uniform(low=-2, high=2, size=1000).reshape((500,2))\n",
        "ys = np.zeros(500)\n",
        "ys[np.logical_and(xs[:,0]>0, xs[:,1]>0)]=1\n",
        "ys[np.logical_and(xs[:,0]<0, xs[:,1]<0)]=1\n",
        "\n",
        "xs = np.asmatrix(xs)\n",
        "ys = np.asmatrix(ys).reshape((500,1))\n",
        "\n",
        "trajectories_momentum, losses_momentum = gradient_descent_momentum(xs, ys, params_init, iterations=3000, lr=1e-7, alpha=0.8)\n",
        "\n",
        "plt.plot(losses_momentum)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Gradient Descent with Momentum\")\n",
        "plt.show()"
      ],
      "id": "c02579ad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.plot(losses_standard, label=\"Gradient Descent\")\n",
        "plt.plot(losses_momentum, label=\"Gradient Descent with Momentum\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Vanilla Gradient Descent v. Gradient Descent with Momentum\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "a8d8505b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(trajectories_momentum)"
      ],
      "id": "399865ad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your code for (F) here\n",
        "\n",
        "from matplotlib import colors\n",
        "\n",
        "Xs = np.random.uniform(low=-2.0, high=2.0, size=1000).reshape((1000,1))\n",
        "Ys = np.random.uniform(low=-2.0, high=2.0, size=1000).reshape((1000,1))\n",
        "Zs = np.logical_xor(Xs<0, Ys>0).reshape((1000,1))\n",
        "\n",
        "XOR = np.concatenate((Xs, Ys, Zs), axis=1)\n",
        "\n",
        "x = np.linspace(-2, 2, 250).reshape(250,1)\n",
        "y = np.linspace(-2, 2, 250).reshape(250,1)\n",
        "Z = np.zeros(250*250).reshape(250,250)\n",
        "\n",
        "\n",
        "W_final, b_final, v_final, c_final = trajectories_standard\n",
        "\n",
        "\n",
        "def ff_nn_relu(X, W, b, v, c):\n",
        "    \"\"\"Computes yhat given input data and model specification\"\"\"\n",
        "    z = np.dot(X, W) + b.reshape(1, -1) # Linear step\n",
        "    a = relu(z) # Activation step\n",
        "    y_hat = sig(np.dot(a, v) + c) # Output step\n",
        "    return y_hat\n",
        "\n",
        "# Given the XOR dataset, compute the decision boundary learned by the network\n",
        "for countx, i in enumerate(x):\n",
        "    for county, j in enumerate(y):\n",
        "        temp = np.array([i[0],j[0]])\n",
        "        Z[countx][county] = ff_nn_relu(temp, W_final, b_final, v_final, c_final) # Your code here\n",
        "\n",
        "\n",
        "X, Y = np.meshgrid(x, y)\n",
        "\n",
        "cmap = colors.ListedColormap(['green', 'red'])\n",
        "plt.contourf(X, Y, Z, cmap=cmap)\n",
        "plt.scatter(Xs, Ys, c=Zs)\n",
        "plt.show()"
      ],
      "id": "15521553",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your code for (F) here\n",
        "\n",
        "from matplotlib import colors\n",
        "\n",
        "Xs = np.random.uniform(low=-2.0, high=2.0, size=1000).reshape((1000,1))\n",
        "Ys = np.random.uniform(low=-2.0, high=2.0, size=1000).reshape((1000,1))\n",
        "Zs = np.logical_xor(Xs<0, Ys>0).reshape((1000,1))\n",
        "\n",
        "XOR = np.concatenate((Xs, Ys, Zs), axis=1)\n",
        "\n",
        "x = np.linspace(-2, 2, 250).reshape(250,1)\n",
        "y = np.linspace(-2, 2, 250).reshape(250,1)\n",
        "Z = np.zeros(250*250).reshape(250,250)\n",
        "\n",
        "\n",
        "W_final, b_final, v_final, c_final = trajectories_momentum\n",
        "\n",
        "\n",
        "def ff_nn_relu(X, W, b, v, c):\n",
        "    \"\"\"Computes yhat given input data and model specification\"\"\"\n",
        "    z = np.dot(X, W) + b.reshape(1, -1) # Linear step\n",
        "    a = relu(z) # Activation step\n",
        "    y_hat = sig(np.dot(a, v) + c) # Output step\n",
        "    return y_hat\n",
        "\n",
        "# Given the XOR dataset, compute the decision boundary learned by the network\n",
        "for countx, i in enumerate(x):\n",
        "    for county, j in enumerate(y):\n",
        "        temp = np.array([i[0],j[0]])\n",
        "        Z[countx][county] = ff_nn_relu(temp, W_final, b_final, v_final, c_final) # Your code here\n",
        "\n",
        "\n",
        "X, Y = np.meshgrid(x, y)\n",
        "\n",
        "cmap = colors.ListedColormap(['green', 'red'])\n",
        "plt.contourf(X, Y, Z, cmap=cmap)\n",
        "plt.scatter(Xs, Ys, c=Zs)\n",
        "plt.show()"
      ],
      "id": "d7b1ad9e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Problem-2** One of the challenges in training neural models is when inputs are not on the same scales. Why is this problematic? Consider the expression for the derivative of the loss with respect to a weight for a particular layer.\n",
        "\n",
        "**YOUR ANSWER HERE**:\n",
        "\n",
        "If inputs arent on the same scale then a gradients wont be the on the same scale either. This could make the learning process unstable as well as maybe being stuck in a local minimum. Additionally, the scale variance could negatively impact how quickly it converges if it is able to converge at all. Lastly, I could see this causing the model to \n"
      ],
      "id": "f82ccdc3"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}